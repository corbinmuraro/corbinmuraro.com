---
layout: blog
title: "Why Voice UI Isn‘t the Future"
date: February 23, 2017
---

# Why Voice UI Isn‘t the Future

I broke the display of my iPhone a few days ago, and for the past few days I have since been relying solely on Siri to interface with my phone. What began as frustration inadvertently got me thinking about the future of UI and what role voice UI might play.

## Background

Film and TV has been thinking about the future of human-computer interaction for a long time. Starfleet computers from Star Trek extensively used voice UI.

We're moving towards more and more abstraction:
1. Tactile buttons that do as you expect (image of old landline)
2. Physical input devices to do a variety of things (keyboard and mouse)
3. Touchscreens (varying input to do a variety of things)
4. Voice UI?

## Pros: 

### Lack of privacy; makes you be more thoughtful with the time you spend with your device

Though conversation may have a lower cognitive load than tapping away on a touchscreen, there is a lack of privacy 


Additionally, speaking words reinforces meaning in a natural way that simply isn't possible when navigating menus. The relationship between "Play Blood by the Middle East" and playing the song "Blood" by the "Middle East" is fantastically intuitive in a way that touchscreens can never come close to.


However, this leads into the main downside and most frustrating part of my experience.


## Cons: 

### You have to relinquish control

Leisurely browsing through content through speech is a tedious and frustrating task. Our visual system allow us to process lots of information at once; our auditory system can't keep up.

Hence, a speech UI can't browse through songs or Google search results in the manner possible on a touchscreen. In exchange, **users will be forced to trust the AI**. Instead of listening to lists of albums or scrolling through Yelp reviews, users will need to trust that the algorithms will choose the same Google result that you would or pick the same Kendrick Lamar song that you would. And of course, the AI needs to be accurate enough to meet these expectations.

### Your memory isn't that good

Even if you're not in the mood to browse through content, **the lack of a display restricts the number of options to what you can recall from memory**; there are no strong cues to help you figure things out. I often found myself asking for the same couple of commands. You forget what options exist when you lose all visual cues. In order to fill in the gaps in our faulty memory without a display, the AI needs to be smart enough to replicate what you would choose to do on a display.

### What about photos and videos?

In the short term, removing a screen removes the ability for a user to consume media content. While I found this restriction very liberating with a mindset similar to backers of the anti-tech dependence [Light Phone](https://www.kickstarter.com/projects/thelightphone/the-light-phone), losing media consumption capability is a big mark against a speech UI in 2017.

When AR becomes viable and mainstream, augmented reality's ability to display visual content could fill in these gaps.

### Nothing to click results in excessive waiting / frustration with results

Perhaps this could be achieved with perfect machine learning to understand pace / what you want to hear...

CALL AND RESPONSE IS SUPER INEFFICIENT (next, next, next...)

ONLY WORKS WELL WHEN YOU SAY ONE COMMAND AND AI DOES THE REST (e.g. play something new; read me my emails)

OR: Physical buttons + speech interface?

	- fast-forward / rewind buttons

	- "next item" button?

	- select button

	- button to queue notifications / get updated