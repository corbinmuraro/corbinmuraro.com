---
layout: blog
title: "Corbin Muraro"
---

# The Future of Voice UI

I broke the display of my iPhone a few days ago, and for the past few days I have since been relying purely on Siri to interface with my phone. I thought it would be a miserable experience, and though it has often been clunky and inefficient, it has gotten me thinking about the future of UI. A purely voice-based interface (like that in [Her](https://en.wikipedia.org/wiki/Her_(film))) may be the right move and is perhaps the most natural next step from smartphones of the present.

## Pros: 

### Lack of privacy; makes you be more thoughtful with the time you spend with your device

Though conversation may have a lower cognitive load than tapping away on a touchscreen, there is a lack of privacy 


Additionally, speaking words reinforces meaning in a natural way that simply isn't possible when navigating menus. The relationship between "Play Blood by the Middle East" and playing the song "Blood" by the "Middle East" is fantastically intuitive in a way that touchscreens can never come close to.


However, this leads into the main downside and most frustrating part of my experience.


## Cons: 

### You have to relinquish control

Leisurely browsing through content through speech is a tedious and frustrating task. Our visual system allow us to process lots of information at once; our auditory system can't keep up.

Hence, a speech UI can't browse through songs or Google search results in the manner possible on a touchscreen. In exchange, **users will be forced to trust the AI**. Instead of listening to lists of albums or scrolling through Yelp reviews, users will need to trust that the algorithms will choose the same Google result that you would or pick the same Kendrick Lamar song that you would. And of course, the AI needs to be accurate enough to meet these expectations.

### Your memory isn't that good

Even if you're not in the mood to browse through content, **the lack of a display restricts the number of options to what you can recall from memory**; there are no strong cues to help you figure things out. I often found myself asking for the same couple of commands. You forget what options exist when you lose all visual cues. In order to fill in the gaps in our faulty memory without a display, the AI needs to be smart enough to replicate what you would choose to do on a display.

### What about photos and videos?

In the short term, removing a screen removes the ability for a user to consume media content. While I found this restriction very liberating with a mindset similar to backers of the anti-tech dependence [Light Phone](https://www.kickstarter.com/projects/thelightphone/the-light-phone), losing media consumption capability is a big mark against a speech UI in 2017.

When AR becomes viable and mainstream, the combination of a audio interface with 

### Nothing to click results in excessive waiting / frustration with results

Perhaps this could be achieved with perfect machine learning to understand pace / what you want to hear...

CALL AND RESPONSE IS SUPER INEFFICIENT (next, next, next...)

ONLY WORKS WELL WHEN YOU SAY ONE COMMAND AND AI DOES THE REST (e.g. play something new; read me my emails)

OR: Physical buttons + speech interface?

	- fast-forward / rewind buttons

	- "next item" button?

	- select button

	- button to queue notifications / get updated